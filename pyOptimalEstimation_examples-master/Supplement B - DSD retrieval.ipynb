{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplement B\n",
    "# Drop Size Distribution retrieval\n",
    "\n",
    "\n",
    "In the following, we introduce the use of the pyOptimalEstimation Python library using a radar-based drop-size distribution (DSD) retrieval as an example. \n",
    "Williams et al. (2014) suggested to parameterize a DSD using a scaling parameter $N_w$ as well as the raindrop mass spectrum mean diameter $D_m$ and its standard deviation $\\sigma_m$.\n",
    "Assuming that the DSD follows a gamma distribution, $\\sigma_m$ is related to the shape parameter $\\mu$ with \n",
    "\\begin{equation} \n",
    "    \\mu = (D_m/\\sigma)^2 -4\n",
    "\\end{equation}\n",
    "and we reconstruct the original DSD $N(D)$ as a function of maximum dimension $D$ with\n",
    "\\begin{equation}\n",
    "    N(D) = N_w \\frac{6}{256} \\frac{(4+\\mu)^{\\mu+4}}{\\Gamma(\\mu+4)} \\Big(\\frac{D}{D_m}\\Big)^{\\mu} \\exp \\Big[ -(4+\\mu) \\frac{D}{D_m}\\Big]\n",
    "\\end{equation}\n",
    "using the normalization approach by Testud et al. (2001).\n",
    "\n",
    "Williams et al. (2014) analyzed the correlations between $D_m$ and $\\sigma_m$ in  detail and proposed to use a modified quantity $\\sigma_m\\!'$ defined with \n",
    "\\begin{equation}\n",
    "    \\sigma_m\\!' = \\sigma_m D_m^{-1.36}\n",
    "\\end{equation}\n",
    "in order to minimize the correlation to $D_m$. The three parameters required to estimate the DSD ($N_w$, $D_m$, and $\\sigma_m\\!'$) will form our state vector $\\mathbf{x}$. \n",
    "\n",
    "\n",
    "A radar simulator will use $\\mathbf{x}$ to estimate the measurement vector $\\mathbf{y}$ consisting of the radar reflectivity factor ($Z_e$ in dBz) and the mean Doppler velocity ($V_d$ in m s$^{-1}$) with \n",
    "    \\begin{equation}\n",
    "    Z_e = 10 \\times \\log_{10} (10^{18} \\times \\sum_i N(D_i) \\times \\frac{\\lambda^4}{|K_w|^2 \\pi^5} \\times \\sigma(D_i) \\times \\Delta D ) \n",
    "    \\end{equation}  \n",
    "and\n",
    "        \\begin{equation}\n",
    "V_d = \\frac{\\sum_i v(D_i) \\times N(D_i) \\times  \\sigma(D_i) \\times \\Delta D}{\\sum_i N(D_i) \\times  \\sigma(D_i) \\times \\Delta D}\n",
    "    \\end{equation}\n",
    "where $i$ the index of the discrete DSD and $\\sigma$ and $v$ are the functions to describe backscattering cross section and fall velocity, respectively. This means we have two measurements to derive three quantities of the state space, i.e. the problem is not unambiguous. \n",
    "In order to obtain the required prior data set, we use the Huntsville data set introduced in Williams et al. (2014) which consists of 18969 individual DSD distributions obtained at Huntsville, AL from December 2009 to October 2011. For every DSD, Williams et al. (2014) determined $N_w$, $D_m$, and $\\sigma_m$ and we determine the a-priori assumption $\\mathbf{x}_a$ and uncertainty (**S**$_a$) form the mean and the covariance of the data set, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation \n",
    "**You can skip the installation if you execute this online on binder**\n",
    "\n",
    "In order to design a retrieval with pyOptimalEstimation, we need to install the library first with \n",
    "~~~~  \n",
    "pip install pyOptimalEstimation\n",
    "~~~~ \n",
    "for Python 2.7 or Python 3. This example uses Python 3.6. It requires the numpy, scipy, pandas and matplotlib libraries to be installed, this example makes use also of the xarray, seaborn, uncertainties, and xarray libraries. For development, we used Numpy 1.18.1, Matplotlib 3.0.3, Scipy 1.2.1, Xarray 0.15.1, Pandas 1.0.3, Seaborn 0.9.0 and Uncertainties 3.0.3.\n",
    "\n",
    "The radar simulations are done with the Passive and Active Microwave radiative TRAnsfer model (PAMTRA, Mech et al. 2020). Here, we use version 2 that is in active development, so we install a specific version with:\n",
    "~~~~ \n",
    "pip install git+git://github.com/maahn/pamtra2.git@461b711ae366b791665687de5dec37abd7ab78f5\n",
    "~~~~ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation\n",
    "\n",
    "We start with loading the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import pandas as pn\n",
    "import xarray as xr\n",
    "import uncertainties.unumpy as unp\n",
    "import seaborn as sns\n",
    "\n",
    "import pamtra2\n",
    "\n",
    "import pyOptimalEstimation as pyOE\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style='ticks', palette='deep')\n",
    "abc = 'abcdefghijklmopqrstuvwxyz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We put some helper routines in a separate file which we load with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('lib')\n",
    "import supporting_routines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State vector\n",
    "\n",
    "In order to load the required prior data set, we load the Huntsville data set of Williams et al. (2014) from a netCDF file using xarray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rainDat = xr.open_dataset('data/huntsville_parameters.nc')[[\n",
    "    'Dm', 'Nw', 'Smprime'\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and add logarithmic variables with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rainDat['Nw_log10'] = np.log10(rainDat['Nw'])\n",
    "rainDat['Dm_log10'] = np.log10(rainDat['Dm'])\n",
    "rainDat['Smprime_log10'] = np.log10(rainDat['Smprime'])\n",
    "\n",
    "rainDat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 100 randomly chosen DSDs for the test data set and use the remaining DSDs for the training data set, i.e. to develop the retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nTest = 100\n",
    "\n",
    "np.random.seed(10)\n",
    "ii = np.arange(len(rainDat.time))\n",
    "np.random.shuffle(ii)\n",
    "testII, trainingII = ii[:nTest], ii[nTest:]\n",
    "\n",
    "rainDat_test = rainDat.isel(time=testII)\n",
    "rainDat_training = rainDat.isel(time=trainingII)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to asses the assumptions that the state variables follow a Gaussian distribution, we make quantile-quantile (QQ) plots. Comparison of the first and second column reveals that a logarithmic state vector $\\mathbf{x}$ is much closer to a Gaussian distribution than the linear version. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 2\n",
    "fig, sps = plt.subplots(figsize=(6.6, 5), nrows=3, ncols=ncols, sharex=True)\n",
    "\n",
    "sps = sps.reshape((3, ncols))\n",
    "\n",
    "jj = 0\n",
    "\n",
    "for kk in range(3):\n",
    "    for ii, (x_name, x_vars) in enumerate([\n",
    "        ('linear', ['Dm', 'Nw', 'Smprime']),\n",
    "        ('log$_{10}$', ['Dm_log10', 'Nw_log10', 'Smprime_log10']),\n",
    "    ]):\n",
    "\n",
    "        x_var = x_vars[kk]\n",
    "        data = (rainDat_training[x_var] - np.mean(\n",
    "            rainDat_training[x_var])) / np.std(rainDat_training[x_var])\n",
    "        (osm, osr), (slope, intercept, r) = stats.probplot(\n",
    "            data, dist=\"norm\", plot=sps[kk, ii], fit=True, rvalue=False)\n",
    "\n",
    "        sps[kk, ii].set_title('')\n",
    "        if kk == 0:\n",
    "            sps[kk, ii].set_title(\n",
    "                'Prior data set: %s $\\mathbf{x}$' % (x_name),\n",
    "                loc='left')\n",
    "\n",
    "        sps[kk, ii].text(\n",
    "            0.05,\n",
    "            0.95,\n",
    "            '%s, R$^2$=%.2f' % (\n",
    "                supporting_routines.niceKeys[x_var], r**2\n",
    "                #, x_mean, x_std\n",
    "            ),\n",
    "            horizontalalignment='left',\n",
    "            verticalalignment='top',\n",
    "            transform=sps[kk, ii].transAxes)\n",
    "\n",
    "        if kk < 2:\n",
    "            sps[kk, ii].set_xlabel(\"\")\n",
    "        if ii > 0:\n",
    "            sps[kk, ii].set_ylabel(\"\")\n",
    "        jj += 1\n",
    "\n",
    "fig.subplots_adjust(hspace=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prior $\\mathbf{x}_{a}$ and covariance **S**$_a$ of the state vector are estimated from the training data set. For convenience, we estimate it for the linear and logarithmic variables together and select the required quantities later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_a = rainDat_training.to_dataframe().cov()\n",
    "x_ap = rainDat_training.to_dataframe().mean()\n",
    "print(x_ap)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(\n",
    "    S_a[\n",
    "        ['Dm', 'Nw', 'Smprime']\n",
    "    ].loc[\n",
    "        ['Dm', 'Nw', 'Smprime']\n",
    "    ],\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    linewidths=.05)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(\n",
    "    S_a[\n",
    "        ['Dm_log10', 'Nw_log10', 'Smprime_log10']\n",
    "    ].loc[\n",
    "        ['Dm_log10', 'Nw_log10', 'Smprime_log10']\n",
    "    ],\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    linewidths=.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When estimating the prior, the use of SI units is not always recommended: Combining small and large quantities with different units (e.g. temperature in K and specific humidity in kg/kg), the prior can be singular prohibiting an efficient use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rainDat_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measurement space\n",
    "\n",
    "The measurement vector $\\mathbf{y}$ consists of the radar reflectivity factor ($Z$) and the mean Doppler velocity ($V_d$) at 13.6 GHz and 35.5 GHz. Even though we use only 13.6 GHz in the following, we encourage the reader to experiment with different $\\mathbf{y}$ combinations. For  **S**$_y$, we assume that the uncertainties of  $Z$ and $V_d$ are 1 dB and 0.3 m s$^{-1}$, respectively, and that the errors are not correlated. Even though these error estimates are generally realistic, the values should be carefully evaluated for a real world application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vars = [\"Ze_13.6\", \"MDV_13.6\", \"Ze_35.5\", \"MDV_35.5\"]\n",
    "y_noise = np.array([1, .3, 1, .3])\n",
    "S_y = pn.DataFrame(\n",
    "    np.diag(y_noise**2),\n",
    "    index=y_vars,\n",
    "    columns=y_vars,\n",
    ")\n",
    "sns.heatmap(S_y, annot=True, fmt='g', linewidths=.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward operator\n",
    "To prepare the retrieval, we set up PAMTRA2 properly and create dictionaries to store the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pam2Prepared = supporting_routines.preparePamtra(frequencies=[13.6e9, 35.5e9])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The message about `relativePermittivityIce` can be ignored. After this, the forward operator can be run with `supporting_routines.forwardPamtra`.  In short, the forward operator accepts $\\mathbf{x}$ as an input argument and returns $\\mathbf{y}$. In addition, `pam2Prepared` (prepared above) and the list of $\\mathbf{y}$ elements to be returned needs to be provided. See `libs/supporting_routines.py` for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example retrieval\n",
    "\n",
    "First, we need names for the elements of $\\mathbf{x}$  and $\\mathbf{y}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vars = ['Dm', 'Nw', 'Smprime']\n",
    "y_vars = [\"Ze_13.6\", \"MDV_13.6\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional  variables required for the forward operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forwardKwArgs = {'pam2': pam2Prepared, 'y_vars': y_vars}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the 51st profile from the test data set as $\\mathbf{x}_{truth}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = 51\n",
    "x_truth = rainDat_test[x_vars].to_dataframe().iloc[tt]\n",
    "x_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we use synthetic observations, i.e. we use PAMTRA2 to simulate an observation based on a known atmospheric state $\\mathbf{x}_{truth}$. Even though $\\mathbf{x}_{truth}$ is typically unknown in a real-world example, the use of synthetic observations is extremely helpful for assessing retrieval performance by comparing $\\mathbf{x}_{op}$ to $\\mathbf{x}_{truth}$. The `**forwardKwArgs` passes all dictionary values of `forwardKwArgs` as named function arguments (see https://www.geeksforgeeks.org/args-kwargs-python/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supporting_routines.forwardPamtra(\n",
    "    x_truth, pam2=pam2Prepared, y_vars=y_vars\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_obs = supporting_routines.forwardPamtra(\n",
    "    x_truth,\n",
    "    **forwardKwArgs,\n",
    ")\n",
    "y_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pyOptimalEstimation` uses objects. Therefore, we combine all information for the reference run into an object which we name `oe` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create optimal estimation object\n",
    "oe = pyOE.optimalEstimation(\n",
    "    x_vars,  # state variable names\n",
    "    x_ap[x_vars],  # a priori\n",
    "    S_a[x_vars].loc[x_vars],  # a priori uncertainty\n",
    "    y_vars,  # measurement variable names\n",
    "    y_obs,  # observations\n",
    "    S_y[y_vars].loc[y_vars],  # observation uncertainty\n",
    "    supporting_routines.forwardPamtra,  # forward Operator\n",
    "    forwardKwArgs=forwardKwArgs,  # additonal function arguments\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store the true profile in the `oe` object. This is just for convenience, it is not\n",
    "used by the retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oe.x_truth = x_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the retrieval with max. 10 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converged = oe.doRetrieval(maxIter=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It converges after 2 iterations. 1.91 degrees of freedom for signal $d$ (the number of independent information pieces) are retrieved which is shows the good performance of the retrieval given that the maximum $d$ value is 2 because $\\mathbf{y}$ consists of 2 measurements. \n",
    "\n",
    "After convergence, the solution $\\mathbf{x}_{op}$ and the retrieved uncertainty **S**$_{op}$ is available with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(oe.x_op)\n",
    "print(oe.S_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyOptimalEstimation uses Pandas objects not only for the input, but also for the output to make sure the elements of $\\mathbf{x}$ and $\\mathbf{y}$ are not mixed up.\n",
    "\n",
    "For convenience, the 1 $\\sigma$ uncertainties derived from **S**$_{op}$ are also available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(oe.x_op_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$d$ is available as a total as well as per $\\mathbf{x}$ variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total', oe.dgf)\n",
    "print(oe.dgf_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend to apply various statistical tests to the result with the `chiSquareTest()` function. The tests include:\n",
    "\n",
    "1. $\\mathbf{y}_{op}$  agrees with the measurement $\\mathbf{y}_{obs}$ (`Y_Optimal_vs_Observation`, Rodgers, 2000, Sec. 12.3.2) \n",
    "2. the observation $\\mathbf{y}_{obs}$ agrees with the prior **S**$_a$ (`Y_Observation_vs_Prior`, Rodgers, 2000, Sec. 12.3.3.1) \n",
    "3. the retrieved measurement agrees with the prior **S**$_a$ (`Y_Optimal_vs_Prior`, Rodgers, 2000, Sec. 12.3.3.3) \n",
    "4. the retrieved state $\\mathbf{x}_{op}$ agrees with the prior **S**$_a$ (`X_Optimal_vs_Prior`, Rodgers, 2000, Sec. 12.3.3.2) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(oe.chiSquareTest()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we test with `testLinearity()`  whether the forward operator is moderately linear following chapter 5.1 of Rodgers 2000. The analysis for this example is based on $\\mathbf{x}_{truth}$. The $\\chi^2$ value (`trueLinearityChi2`) corresponding to the model beeing moderately linear must be smaller than the critical value (`trueLinearityChi2Crit`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first returned element is a boolean indicating whether the $\\chi^2$ tests were successful: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linearity, trueLinearityChi2, trueLinearityChi2Crit = oe.linearityTest(\n",
    "    significance=0.05)\n",
    "print('Linearity test passed: ', trueLinearityChi2 < trueLinearityChi2Crit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that this test fails indicates already that the retrieval with linear $\\mathbf{x}$ variables does not work properly (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running several retrievals, we recommend using the `oe.summarize` function which summarizes all retrieval results in an Xarray Dataset (which is structured similar to a netCDF file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oe.summarize(returnXarray=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run retrieval\n",
    "\n",
    "After we presented the retrieval step-by-step, we integrate everything into a loop to apply the retrieval to all 100 profiles. \n",
    "\n",
    "The results are saved in dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsOE = {}\n",
    "failed = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare performance, we run the retrieval in two configurations:\n",
    "\n",
    "1. linear state variables (see the step-by-step example above)\n",
    "2. logarithmic state variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     name, [X_variables],\n",
    "x_names = dict([\n",
    "    ('Linear', ['Dm', 'Nw', 'Smprime']),\n",
    "    ('log$_{10}$', ['Dm_log10', 'Nw_log10', 'Smprime_log10']),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this study, we use only a measurement configuration consisting of a radar reflectivity ($Z_e$ in dBz) and the mean Doppler velocity ($V_d$ in m s$^{-1}$). But the example is designed to work also with dual-frequency set ups and we encourage the reader to experiment  with different configurations. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     name, [Y_variables],\n",
    "y_names = dict([\n",
    "    ('Z', [\"Ze_13.6\"]),\n",
    "    ('ZW', [\"Ze_13.6\", \"MDV_13.6\"]),\n",
    "    ('Zdual', [\"Ze_13.6\", \"Ze_35.5\"]),\n",
    "    ('ZWdual', [\"Ze_13.6\", \"MDV_13.6\", \"Ze_35.5\", \"MDV_35.5\"]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for x_name, y_name in (\n",
    "    (\n",
    "        'Linear',\n",
    "        'ZW',\n",
    "    ),\n",
    "    ('log$_{10}$', 'ZW'),\n",
    "):\n",
    "\n",
    "    x_vars = x_names[x_name]\n",
    "    y_vars = y_names[y_name]\n",
    "\n",
    "    resultsOE['%s %s' % (x_name, y_name)] = []\n",
    "    failed['%s %s' % (x_name, y_name)] = []\n",
    "\n",
    "    #Additional required variables for the forward operator\n",
    "    forwardKwArgs = {'pam2': pam2Prepared, 'y_vars': y_vars}\n",
    "\n",
    "    print('#' * 80)\n",
    "    print(x_name, y_name)\n",
    "    print('#' * 80)\n",
    " \n",
    "    #     for tt in [15, 20, 51]:\n",
    "    for tt in range(rainDat_test[x_vars].time.shape[0]):\n",
    "\n",
    "        x_truth = rainDat_test[x_vars].to_dataframe().iloc[tt]\n",
    "        # simulate observation based on true state x_truth\n",
    "        y_obs = supporting_routines.forwardPamtra(\n",
    "            x_truth,\n",
    "            pam2=pam2Prepared,\n",
    "            y_vars=y_vars,\n",
    "        )\n",
    "\n",
    "        # create optimal estimation object\n",
    "        oe = pyOE.optimalEstimation(\n",
    "            x_vars,  # state variable names\n",
    "            x_ap[x_vars],  # a priori\n",
    "            S_a[x_vars].loc[x_vars],  # a priori uncertainty\n",
    "            y_vars,  # measurement variable names\n",
    "            y_obs,  # observations\n",
    "            S_y[y_vars].loc[y_vars],  # observation uncertainty\n",
    "            supporting_routines.forwardPamtra,  # forward Operator\n",
    "            forwardKwArgs=forwardKwArgs,  # additonal function arguments\n",
    "        )\n",
    "\n",
    "        # Store true profile in `oe` object. This is just for convenience, it is not\n",
    "        # used by the retrieval.\n",
    "        oe.x_truth = x_truth\n",
    "\n",
    "        # Do the retrieval with max. 10 iterations.\n",
    "        converged = oe.doRetrieval(maxIter=10)\n",
    "\n",
    "        if converged:\n",
    "\n",
    "            # Test whethe rthe retrieval is moderately lienar around x_truth\n",
    "            print(oe.linearityTest())\n",
    "\n",
    "            # Show hdegrees of freedom per variable\n",
    "            print(oe.dgf_x)\n",
    "\n",
    "            # Apply chi2 tests for retrieval quality\n",
    "            print(oe.chiSquareTest())\n",
    "\n",
    "            if x_name.startswith('log'):\n",
    "                # Show RMS normalized with prior\n",
    "                print(\n",
    "                    'RMS log',\n",
    "                    np.sqrt(\n",
    "                        np.mean(\n",
    "                            ((10**oe.x_truth - 10**oe.x_op) / 10**oe.x_a)**2)))\n",
    "                print('truth', 10**oe.x_truth)\n",
    "                print('op', 10**unp.uarray(oe.x_op.values, oe.x_op_err.values))\n",
    "            else:\n",
    "                # Show RMS normalized with prior\n",
    "                print('RMS',\n",
    "                      np.sqrt(np.mean(((oe.x_truth - oe.x_op) / oe.x_a)**2)))\n",
    "                print('truth', oe.x_truth)\n",
    "                print('op', unp.uarray(oe.x_op.values, oe.x_op_err.values))\n",
    "\n",
    "            #Store results in xarray DataArray\n",
    "            summary = oe.summarize(returnXarray=True)\n",
    "            summary['time'] = rainDat_test[x_vars].time[tt]\n",
    "\n",
    "            resultsOE['%s %s' % (x_name, y_name)].append(summary)\n",
    "        else:\n",
    "            failed['%s %s' % (x_name, y_name)].append(tt)\n",
    "\n",
    "    #Store results in xarray Dataset structure for later analysis.\n",
    "    resultsOE['%s %s' % (x_name, y_name)] = xr.concat(\n",
    "        resultsOE['%s %s' % (x_name, y_name)], dim='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare retrieval quality\n",
    "\n",
    "Here we compare the retrieval quality by comparing the number of profiles that converged, passed the $\\chi^2$-tests, and the linearity test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "fig, sps = plt.subplots(ncols=3, sharey=True, figsize=(6, 4))\n",
    "\n",
    "for kk, key in enumerate(resultsOE.keys()):\n",
    "\n",
    "    converged = len(resultsOE[key].time)\n",
    "    passedLinearity = (resultsOE[key].trueLinearityChi2 <\n",
    "                       resultsOE[key].trueLinearityChi2Critical).sum().values\n",
    "    passedChi2 = (resultsOE[key].chi2value <\n",
    "                  resultsOE[key].chi2critical).all('chi2test').sum().values\n",
    "\n",
    "    sps[0].bar(kk, converged, label=key, width=0.98)\n",
    "    sps[1].bar(kk, passedChi2, label=key, width=0.98)\n",
    "    sps[2].bar(kk, passedLinearity, label=key, width=0.98)\n",
    "\n",
    "    sps[0].text(\n",
    "        kk, converged + .25, str(converged), horizontalalignment='center')\n",
    "    sps[1].text(\n",
    "        kk, passedChi2 + .25, str(passedChi2), horizontalalignment='center')\n",
    "    sps[2].text(\n",
    "        kk,\n",
    "        passedLinearity + .25,\n",
    "        str(passedLinearity),\n",
    "        horizontalalignment='center')\n",
    "\n",
    "    sps[0].set_xticks([])\n",
    "    sps[1].set_xticks([])\n",
    "    sps[2].set_xticks([])\n",
    "    sps[0].set_xlabel('converged')\n",
    "    sps[2].set_xlabel('passed linear tests')\n",
    "    sps[1].set_xlabel('passed all $\\chi^2$ tests')\n",
    "\n",
    "    sps[0].set_ylabel('runs [%]')\n",
    "    sps[0].set_ylim(0, 110)\n",
    "    sps[0].legend(loc='lower center')\n",
    "\n",
    "    for ii in range(3):\n",
    "        sps[ii].text(\n",
    "            0.05,\n",
    "            0.97,\n",
    "            '%s)' % (abc[ii]),\n",
    "            horizontalalignment='left',\n",
    "            verticalalignment='top',\n",
    "            transform=sps[ii].transAxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Independently how performance is measured, the version with the logarithmic $\\mathbf{x}$ variables performs better. It should be noted that applying a $\\chi^2$ test with 5% confidence interval to a larger number of retrievals means that 5% of the profiles do not pass the test even though they are fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare retrieval error distributions\n",
    "\n",
    "Similar to the prior, we also compare the distributions of relative retrieval errors defined with\n",
    "\n",
    "   \\begin{equation}\n",
    "   \\frac{\\mathbf{x}_{op} - \\mathbf{x}_{truth}}{\\sqrt{(\\textrm{diag}( \\mathsf{S}_y)}}\n",
    "   \\end{equation}\n",
    "\n",
    "In theory, the relative retrieval errors should follow a normal distribution with mean 0 and standard deviation 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = len(resultsOE.keys())\n",
    "fig, sps = plt.subplots(\n",
    "    figsize=(3.33 * ncols, 5), nrows=len(x_vars), ncols=ncols, sharex=True)\n",
    "\n",
    "sps = sps.reshape((len(x_vars), ncols))\n",
    "\n",
    "jj = 0\n",
    "\n",
    "for yy, key in enumerate(resultsOE.keys()):\n",
    "\n",
    "    for kk in range(3):\n",
    "\n",
    "        x_var = resultsOE[key].x_vars.values[kk]\n",
    "        thisOe = resultsOE[key].isel(x_vars=kk)\n",
    "        passedChiTests = (thisOe.chi2value <=\n",
    "                          thisOe.chi2critical).all('chi2test')\n",
    "        thisOe = thisOe.where(passedChiTests).dropna('time')\n",
    "        x_err = ((thisOe['x_op'] - thisOe['x_truth']) / thisOe['x_op_err'])\n",
    "\n",
    "        x_mean = x_err.mean().values\n",
    "        x_std = x_err.std().values\n",
    "\n",
    "        print(key, x_var, x_mean, x_std)\n",
    "\n",
    "        (osm, osr), (slope, intercept, r) = stats.probplot(\n",
    "            x_err, dist=\"norm\", plot=sps[kk, yy], fit=True, rvalue=False)\n",
    "\n",
    "        sps[kk, yy].set_title('')\n",
    "        if kk == 0:\n",
    "            sps[kk, yy].set_title(\n",
    "                'Posterior data set: %s $\\mathbf{x}$' %\n",
    "                (key.split(' ')[0]),\n",
    "                loc='left')\n",
    "\n",
    "        sps[kk, yy].text(\n",
    "            0.95,\n",
    "            0.05,\n",
    "            '%s, R$^2$=%.2f\\nmean=%.2f, std=%.2f' %\n",
    "            (supporting_routines.niceKeys[x_var], r**2, x_mean, x_std),\n",
    "            horizontalalignment='right',\n",
    "            verticalalignment='bottom',\n",
    "            transform=sps[kk, yy].transAxes)\n",
    "\n",
    "        if kk < 2:\n",
    "            sps[kk, yy].set_xlabel(\"\")\n",
    "        sps[kk, yy].set_ylabel(\"\")\n",
    "\n",
    "        jj += 1\n",
    "\n",
    "fig.subplots_adjust(hspace=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With one exception, mean and standard deviation are closer to 0 and 1, respectively, for the logarithmic retrieval. In other words, non-normally distributed state variables lead to non-normally distributed retrieval error distributions. In this case, the retrieval uncertainty is not correctly described with **S**$_{op}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare retrieval information content\n",
    "\n",
    "Here, we compare the information content of the two retrieval versions. For this we use the optimal to prior uncertainty ratio (defined as sqrt[diag(**S**$_{op}$)/diag(**S**$_a$)]) and the individual *d* for all profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, [ax1, ax2] = plt.subplots(ncols = 2, figsize=(6,3))\n",
    "positions = np.array([0,2,4])\n",
    "                     \n",
    "\n",
    "\n",
    "for yy, key in enumerate(resultsOE.keys()):\n",
    "\n",
    "    thisOE = resultsOE[key].where((resultsOE[key].chi2value < resultsOE[key].chi2critical).all('chi2test'))\n",
    "    \n",
    "    dat = (thisOE['x_op_err']/thisOE['x_a_err']).to_pandas().dropna() * 100\n",
    "    try: #Different Matplotlib versions handle this differently\n",
    "        ax1.violinplot(dat, positions = positions+yy, showmedians=True)\n",
    "    except ValueError:\n",
    "        ax1.violinplot(dat.T, positions = positions+yy, showmedians=True)\n",
    "\n",
    "    dat = (thisOE['dgf_x']).to_pandas().dropna()\n",
    "    try: #Different Matplotlib versions handle this differently\n",
    "        ax2.violinplot(dat, positions = positions+yy, showmedians=True)\n",
    "    except ValueError:\n",
    "        ax2.violinplot(dat.T, positions = positions+yy, showmedians=True)\n",
    "\n",
    "    \n",
    "cols = ['Dm', 'Dm_log10', 'Nw', 'Nw_log10', 'Smprime', 'Smprime_log10']\n",
    "labels = []\n",
    "for tt in cols:\n",
    "    labels.append(supporting_routines.niceKeys[tt].replace('log','\\nlog'))\n",
    "ax2.set_xticks(range(6)) \n",
    "ax2.set_xticklabels(labels) \n",
    "ax1.set_xticks(range(6)) \n",
    "ax1.set_xticklabels(labels) \n",
    "\n",
    "ax1.set_ylabel('optimal to prior\\nuncertainty ratio [%]')\n",
    "ax2.set_ylabel('degrees of freedom\\nfor signal [-]')\n",
    "\n",
    "custom_lines = [matplotlib.lines.Line2D([0], [0], color='C0', lw=4),\n",
    "                matplotlib.lines.Line2D([0], [0], color='C1', lw=4),\n",
    "               ]\n",
    "\n",
    "ax1.legend(custom_lines, ['linear', 'log$_{10}$'], loc='lower right')\n",
    "ax1.text(0.03, 0.97, 'a)', horizontalalignment='left', verticalalignment='top', transform = ax1.transAxes)\n",
    "ax2.text(0.03, 0.03, 'b)', horizontalalignment='left', verticalalignment='bottom', transform = ax2.transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Figure4.png', dpi=200)\n",
    "plt.savefig('Figure4.pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the median values of the optimal to prior uncertainty ratio distribution are lower for the linear state variables indicating that the linear retrieval version is in general underestimating retrieval uncertainties. At the same time, *d* can be greatly overestimated for individual profiles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mech, M., M. Maahn, S. Kneifel, D. Ori, E. Orlandi, P. Kollias, V. Schemann, and S. Crewell, 2020: PAMTRA 1.0: A Passive and Active Microwave radiative TRAnsfer tool for simulating radiometer and radar measurements of the cloudy atmosphere. Geoscientific Model Development Discussions, 1–34, doi:https://doi.org/10.5194/gmd-2019-356.\n",
    "\n",
    "Testud, J., S. Oury, R. A. Black, P. Amayenc, and X. Dou, 2001: The concept of normalized distribution to describe raindrop spectra: A tool for cloud physics and cloud remote sensing. *J. Appl. Meteorol.*, 40, 1118–1140, [https://doi.org/10.1175/1520-0450(2001)040<1118:TCONDT>2.0.CO;2](https://doi.org/10.1175/1520-0450(2001)040<1118:TCONDT>2.0.CO;2).\n",
    "\n",
    "Williams, C. R., V. N. Bringi, L. D. Carey, V. Chandrasekar, P. N. Gatlin, Z. S. Haddad, R. Meneghini, S. Joseph Munchak, S. W. Nesbitt, W. A. Petersen, S. Tanelli, A. Tokay, A. Wilson, and D. B. Wolff, 2014: Describing the Shape of Raindrop Size Distributions Using Uncorrelated Raindrop Mass Spectrum Parameters. *J. Appl. Meteor. Climatol.*, 53, 1282–1296, https://doi.org/10.1175/JAMC-D-13-076.1.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyoe_examples",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "notify_time": "10",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "249px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
