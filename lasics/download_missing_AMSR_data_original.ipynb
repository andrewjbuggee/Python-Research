{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Missing AMSR-E Data for Existing EMIT-Aqua Coincident Pairs\n",
    "\n",
    "**Purpose:**  \n",
    "This notebook scans existing coincident data directories, reads AIRS and MODIS filenames to determine acquisition times, and downloads the corresponding AMSR-E data that was missing from the original download.\n",
    "\n",
    "**Requirements:**\n",
    "+ A NASA [Earthdata Login](https://urs.earthdata.nasa.gov/) account is required\n",
    "+ Configured `.netrc` file with NASA Earthdata credentials\n",
    "+ Existing coincident data directory structure from original download\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "import requests\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set the base directory where your coincident data was downloaded. The notebook will scan subdirectories to find existing AIRS/MODIS files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directory containing your coincident pair subdirectories\n",
    "base_data_dir = '/Users/anbu8374/Downloads/coincident_data/'\n",
    "\n",
    "# CMR API base URL\n",
    "cmrurl = 'https://cmr.earthdata.nasa.gov/search/'\n",
    "\n",
    "# AMSR-E product DOIs and concept IDs\n",
    "amsr_products = {\n",
    "    'AE_Rain': {\n",
    "        'doi': '10.5067/AMSR-E/AE_RAIN_DAY.003',\n",
    "        'concept_id': None,  # Will be fetched\n",
    "        'description': 'AMSR-E/Aqua L2B Global Swath Surface Precipitation'\n",
    "    },\n",
    "    'AE_Ocean': {\n",
    "        'doi': '10.5067/AMSR-E/AE_OCEAN.003',\n",
    "        'concept_id': None,  # Will be fetched\n",
    "        'description': 'AMSR-E/Aqua L2B Global Swath Ocean Products'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Verify base directory exists\n",
    "if not os.path.exists(base_data_dir):\n",
    "    raise ValueError(f\"Base data directory not found: {base_data_dir}\")\n",
    "\n",
    "print(f\"Base data directory: {base_data_dir}\")\n",
    "print(f\"Directory exists: {os.path.exists(base_data_dir)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get AMSR-E Concept IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch concept IDs for AMSR-E products\n",
    "for product_key, product_info in amsr_products.items():\n",
    "    doi = product_info['doi']\n",
    "    doisearch = cmrurl + 'collections.json?doi=' + doi\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(doisearch)\n",
    "        response.raise_for_status()\n",
    "        concept_id = response.json()['feed']['entry'][0]['id']\n",
    "        amsr_products[product_key]['concept_id'] = concept_id\n",
    "        print(f\"{product_key}: {concept_id}\")\n",
    "        print(f\"  Description: {product_info['description']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching concept ID for {product_key}: {e}\")\n",
    "        print(f\"  DOI: {doi}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Existing Data Directories\n",
    "\n",
    "Scan the coincident data directories and extract timing information from AIRS and MODIS filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_aqua_filename(filename):\n",
    "    \"\"\"\n",
    "    Extract date and time from Aqua instrument filenames.\n",
    "    \n",
    "    Examples:\n",
    "    - AIRS: AIRS.2024.05.16.193.L2.RetStd.v7.0.7.0.G24137155634.hdf\n",
    "    - MODIS: MYD021KM.A2024137.1930.061.2024138154624.hdf\n",
    "    - AMSR: AMSR_E_L2A_BrightnessTemperatures_V12_202405161907_D.hdf5\n",
    "    \n",
    "    Returns:\n",
    "        dict: {'year': int, 'month': int, 'day': int, 'hour': int, 'minute': int}\n",
    "              or None if parsing fails\n",
    "    \"\"\"\n",
    "    # AIRS format: AIRS.YYYY.MM.DD.HHH (where HHH is granule number, ~6 min each)\n",
    "    airs_match = re.search(r'AIRS\\.(\\d{4})\\.(\\d{2})\\.(\\d{2})\\.(\\d{3})', filename)\n",
    "    if airs_match:\n",
    "        year = int(airs_match.group(1))\n",
    "        month = int(airs_match.group(2))\n",
    "        day = int(airs_match.group(3))\n",
    "        granule = int(airs_match.group(4))\n",
    "        \n",
    "        # Convert granule number to approximate UTC time\n",
    "        # AIRS has 240 granules per day (6 minute granules)\n",
    "        minutes_since_midnight = granule * 6\n",
    "        hour = minutes_since_midnight // 60\n",
    "        minute = minutes_since_midnight % 60\n",
    "        \n",
    "        return {\n",
    "            'year': year,\n",
    "            'month': month,\n",
    "            'day': day,\n",
    "            'hour': hour,\n",
    "            'minute': minute,\n",
    "            'source': 'AIRS'\n",
    "        }\n",
    "    \n",
    "    # MODIS format: MYD021KM.AYYYYDDD.HHMM\n",
    "    modis_match = re.search(r'MYD\\d{5}\\.A(\\d{4})(\\d{3})\\.(\\d{2})(\\d{2})', filename)\n",
    "    if modis_match:\n",
    "        year = int(modis_match.group(1))\n",
    "        doy = int(modis_match.group(2))  # Day of year\n",
    "        hour = int(modis_match.group(3))\n",
    "        minute = int(modis_match.group(4))\n",
    "        \n",
    "        # Convert day of year to month and day\n",
    "        date = dt.datetime(year, 1, 1) + dt.timedelta(days=doy - 1)\n",
    "        \n",
    "        return {\n",
    "            'year': date.year,\n",
    "            'month': date.month,\n",
    "            'day': date.day,\n",
    "            'hour': hour,\n",
    "            'minute': minute,\n",
    "            'source': 'MODIS'\n",
    "        }\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def create_temporal_search_string(time_info, window_minutes=30):\n",
    "    \"\"\"\n",
    "    Create CMR temporal search string with a time window around the observation.\n",
    "    \n",
    "    Args:\n",
    "        time_info: dict with year, month, day, hour, minute\n",
    "        window_minutes: search window in minutes (default ±30 minutes)\n",
    "    \n",
    "    Returns:\n",
    "        str: CMR temporal search string\n",
    "    \"\"\"\n",
    "    obs_time = dt.datetime(\n",
    "        time_info['year'],\n",
    "        time_info['month'],\n",
    "        time_info['day'],\n",
    "        time_info['hour'],\n",
    "        time_info['minute']\n",
    "    )\n",
    "    \n",
    "    start_time = obs_time - dt.timedelta(minutes=window_minutes)\n",
    "    end_time = obs_time + dt.timedelta(minutes=window_minutes)\n",
    "    \n",
    "    dt_format = '%Y-%m-%dT%H:%M:%SZ'\n",
    "    return start_time.strftime(dt_format) + ',' + end_time.strftime(dt_format)\n",
    "\n",
    "\n",
    "# Scan directories for existing data\n",
    "print(\"Scanning data directories...\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "pair_info = {}  # Dictionary to store info for each pair\n",
    "\n",
    "# Get all subdirectories in base_data_dir\n",
    "subdirs = [d for d in Path(base_data_dir).iterdir() if d.is_dir()]\n",
    "\n",
    "for subdir in sorted(subdirs):\n",
    "    pair_name = subdir.name\n",
    "    \n",
    "    # Check if directory has AIRS or MODIS files but no AMSR files\n",
    "    files = list(subdir.glob('*'))\n",
    "    airs_files = [f for f in files if 'AIRS' in f.name and f.suffix in ['.hdf', '.nc']]\n",
    "    modis_files = [f for f in files if 'MYD' in f.name and f.suffix in ['.hdf', '.nc']]\n",
    "    amsr_files = [f for f in files if 'AMSR' in f.name or 'AE_' in f.name]\n",
    "    \n",
    "    # Skip if no AIRS/MODIS files or if AMSR files already exist\n",
    "    if (not airs_files and not modis_files):\n",
    "        continue\n",
    "    \n",
    "    if amsr_files:\n",
    "        print(f\"✓ {pair_name}: AMSR data already exists ({len(amsr_files)} files) - SKIPPING\")\n",
    "        continue\n",
    "    \n",
    "    # Try to parse timing from AIRS or MODIS files\n",
    "    time_info = None\n",
    "    source_file = None\n",
    "    \n",
    "    # Prefer MODIS for more precise timing\n",
    "    for modis_file in modis_files:\n",
    "        time_info = parse_aqua_filename(modis_file.name)\n",
    "        if time_info:\n",
    "            source_file = modis_file.name\n",
    "            break\n",
    "    \n",
    "    # Fall back to AIRS if MODIS parsing failed\n",
    "    if not time_info:\n",
    "        for airs_file in airs_files:\n",
    "            time_info = parse_aqua_filename(airs_file.name)\n",
    "            if time_info:\n",
    "                source_file = airs_file.name\n",
    "                break\n",
    "    \n",
    "    if time_info:\n",
    "        pair_info[pair_name] = {\n",
    "            'directory': subdir,\n",
    "            'time_info': time_info,\n",
    "            'source_file': source_file,\n",
    "            'temporal_str': create_temporal_search_string(time_info),\n",
    "            'airs_count': len(airs_files),\n",
    "            'modis_count': len(modis_files)\n",
    "        }\n",
    "        \n",
    "        print(f\"✗ {pair_name}: Missing AMSR data\")\n",
    "        print(f\"    Time: {time_info['year']:04d}-{time_info['month']:02d}-{time_info['day']:02d} \"\n",
    "              f\"{time_info['hour']:02d}:{time_info['minute']:02d} UTC (from {time_info['source']})\")\n",
    "        print(f\"    Files: {len(airs_files)} AIRS, {len(modis_files)} MODIS\")\n",
    "    else:\n",
    "        print(f\"⚠ {pair_name}: Could not parse timing information - SKIPPING\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nFound {len(pair_info)} pair(s) missing AMSR data\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search and Download AMSR-E Data\n",
    "\n",
    "For each pair missing AMSR data, search CMR for coincident AMSR-E files and download them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_cmr_amsr(concept_id, temporal_str, page_size=2000):\n",
    "    \"\"\"\n",
    "    Search CMR for AMSR-E granules matching temporal criteria.\n",
    "    \n",
    "    Returns:\n",
    "        list: URLs of matching granules\n",
    "    \"\"\"\n",
    "    granule_search_url = cmrurl + 'granules'\n",
    "    \n",
    "    search_params = {\n",
    "        'concept_id': concept_id,\n",
    "        'temporal': temporal_str,\n",
    "        'page_size': page_size,\n",
    "    }\n",
    "    \n",
    "    headers = {'Accept': 'application/json'}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(granule_search_url, params=search_params, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        granules = response.json()['feed']['entry']\n",
    "        \n",
    "        # Extract data file URLs (exclude metadata and auxiliary files)\n",
    "        urls = []\n",
    "        for g in granules:\n",
    "            file_urls = [\n",
    "                x['href'] for x in g.get('links', [])\n",
    "                if 'https' in x['href']\n",
    "                and any(ext in x['href'] for ext in ['.hdf', '.nc', '.h5', '.he5', '.hdf5'])\n",
    "                and '.dmrpp' not in x['href']\n",
    "                and not any(x['href'].endswith(f'.{digit}') for digit in '0123456789')\n",
    "                and not x['href'].endswith(('.xml', '.qa', '.ph', '.html'))\n",
    "            ]\n",
    "            urls.extend(file_urls)\n",
    "        \n",
    "        return urls\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"    Error searching CMR: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# Search for and download AMSR data for each pair\n",
    "print(\"=\" * 70)\n",
    "print(\"SEARCHING FOR AMSR-E DATA\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "download_summary = defaultdict(list)\n",
    "\n",
    "for pair_name, info in pair_info.items():\n",
    "    print(f\"Pair: {pair_name}\")\n",
    "    print(f\"  Time: {info['temporal_str']}\")\n",
    "    \n",
    "    pair_urls = []\n",
    "    \n",
    "    # Search each AMSR product\n",
    "    for product_key, product_data in amsr_products.items():\n",
    "        if not product_data['concept_id']:\n",
    "            print(f\"  - {product_key}: No concept ID available - SKIPPING\")\n",
    "            continue\n",
    "        \n",
    "        urls = search_cmr_amsr(product_data['concept_id'], info['temporal_str'])\n",
    "        \n",
    "        if urls:\n",
    "            print(f\"  - {product_key}: Found {len(urls)} file(s)\")\n",
    "            pair_urls.extend(urls)\n",
    "        else:\n",
    "            print(f\"  - {product_key}: No files found\")\n",
    "    \n",
    "    if pair_urls:\n",
    "        download_summary[pair_name] = {\n",
    "            'urls': pair_urls,\n",
    "            'directory': info['directory'],\n",
    "            'count': len(pair_urls)\n",
    "        }\n",
    "        print(f\"  Total files to download: {len(pair_urls)}\\n\")\n",
    "    else:\n",
    "        print(f\"  ⚠ No AMSR data found for this time period\\n\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total pairs with AMSR data found: {len(download_summary)}\")\n",
    "print(f\"Total files to download: {sum(v['count'] for v in download_summary.values())}\")\n",
    "print(\"=\" * 70)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download AMSR Files\n",
    "\n",
    "Download the identified AMSR-E files to their respective pair directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"DOWNLOADING AMSR-E FILES\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "total_downloaded = 0\n",
    "total_failed = 0\n",
    "\n",
    "for pair_name, download_info in download_summary.items():\n",
    "    pair_dir = download_info['directory']\n",
    "    urls = download_info['urls']\n",
    "    \n",
    "    print(f\"Downloading to: {pair_name}/\")\n",
    "    print(f\"  Files: {len(urls)}\")\n",
    "    \n",
    "    # Create URL file for wget\n",
    "    url_file = pair_dir / 'amsr_urls_to_download.txt'\n",
    "    with open(url_file, 'w') as f:\n",
    "        for url in urls:\n",
    "            f.write(url + '\\n')\n",
    "    \n",
    "    # Download using wget\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['wget', '-P', str(pair_dir), '-i', str(url_file)],\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        \n",
    "        # Count successful downloads\n",
    "        if result.stderr:\n",
    "            saved_count = result.stderr.count('saved')\n",
    "            total_downloaded += saved_count\n",
    "            print(f\"  ✓ Downloaded {saved_count} file(s)\")\n",
    "        \n",
    "        # Clean up URL file if successful\n",
    "        if result.returncode == 0:\n",
    "            url_file.unlink()\n",
    "        else:\n",
    "            total_failed += len(urls)\n",
    "            print(f\"  ⚠ Download completed with warnings (return code: {result.returncode})\")\n",
    "            print(f\"    URL file saved: {url_file.name}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        total_failed += len(urls)\n",
    "        print(f\"  ✗ Error downloading: {e}\")\n",
    "        print(f\"    URLs saved to: {url_file.name}\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DOWNLOAD COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Successfully downloaded: {total_downloaded} files\")\n",
    "if total_failed > 0:\n",
    "    print(f\"Failed/warnings: {total_failed} files\")\n",
    "print(f\"Data location: {base_data_dir}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The notebook has:\n",
    "1. Scanned your existing coincident data directories\n",
    "2. Identified pairs missing AMSR-E data\n",
    "3. Extracted timing information from AIRS/MODIS filenames\n",
    "4. Searched NASA CMR for matching AMSR-E granules\n",
    "5. Downloaded AMSR-E files to the appropriate directories\n",
    "\n",
    "**Note:** If you encounter download issues, check:\n",
    "- Your `.netrc` file has correct NASA Earthdata credentials\n",
    "- File permissions: `chmod 600 ~/.netrc`\n",
    "- Any `*_urls_to_download.txt` files left in directories indicate partial downloads that can be retried manually"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
