{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Missing MYD021KM Data for Existing EMIT-Aqua Coincident Pairs\n",
    "\n",
    "**Purpose:**  \n",
    "This notebook scans existing coincident data directories, reads AIRS filenames to determine acquisition times, and downloads the corresponding MYD021KM (MODIS Calibrated Radiance) data that was missing from the original download.\n",
    "\n",
    "**Requirements:**\n",
    "+ A NASA [Earthdata Login](https://urs.earthdata.nasa.gov/) account is required\n",
    "+ Configured `.netrc` file with NASA Earthdata credentials\n",
    "+ Existing coincident data directory structure from original download\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "import requests\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set the base directory where your coincident data was downloaded. The notebook will scan subdirectories to find existing AIRS files and use them to determine when to search for MODIS data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base data directory: /Users/andrewbuggee/Documents/MATLAB/Matlab-Research/Hyperspectral_Cloud_Retrievals/Batch_Scripts/Paper-2/coincident_EMIT_Aqua_data/\n",
      "Directory exists: True\n"
     ]
    }
   ],
   "source": [
    "# Base directory containing your coincident pair subdirectories\n",
    "base_data_dir = '/Users/andrewbuggee/Documents/MATLAB/Matlab-Research/Hyperspectral_Cloud_Retrievals/Batch_Scripts/Paper-2/coincident_EMIT_Aqua_data/'\n",
    "\n",
    "# CMR API base URL\n",
    "cmrurl = 'https://cmr.earthdata.nasa.gov/search/'\n",
    "\n",
    "# MYD021KM product information\n",
    "modis_product = {\n",
    "    'MYD021KM': {\n",
    "        'doi': '10.5067/MODIS/MYD021KM.061',\n",
    "        'concept_id': None,  # Will be fetched\n",
    "        'name': 'Aqua/MODIS Level-1B Calibrated Radiances 1km',\n",
    "        'description': 'MODIS/Aqua Calibrated Radiances 5-Min L1B Swath 1km'\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# Combine products\n",
    "all_products = {**modis_product}\n",
    "\n",
    "# Verify base directory exists\n",
    "if not os.path.exists(base_data_dir):\n",
    "    raise ValueError(f\"Base data directory not found: {base_data_dir}\")\n",
    "\n",
    "print(f\"Base data directory: {base_data_dir}\")\n",
    "print(f\"Directory exists: {os.path.exists(base_data_dir)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get MODIS Product Concept IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MYD021KM: C1379758607-LAADS\n",
      "  Aqua/MODIS Level-1B Calibrated Radiances 1km\n"
     ]
    }
   ],
   "source": [
    "# Fetch concept IDs for MODIS products\n",
    "for product_key, product_info in all_products.items():\n",
    "    doi = product_info['doi']\n",
    "    doisearch = cmrurl + 'collections.json?doi=' + doi\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(doisearch)\n",
    "        response.raise_for_status()\n",
    "        concept_id = response.json()['feed']['entry'][0]['id']\n",
    "        all_products[product_key]['concept_id'] = concept_id\n",
    "        print(f\"{product_key}: {concept_id}\")\n",
    "        print(f\"  {product_info['name']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching concept ID for {product_key}: {e}\")\n",
    "        print(f\"  DOI: {doi}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Existing Data Directories\n",
    "\n",
    "Scan the coincident data directories and extract timing information from AIRS filenames. We'll use AIRS timing to search for coincident MODIS data since both are on the Aqua satellite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning data directories...\n",
      "\n",
      "======================================================================\n",
      "✓ 2023_9_16_T191106_2: MYD021KM data already exists (1 files) - SKIPPING\n",
      "✓ 2023_9_16_T191106_3: MYD021KM data already exists (3 files) - SKIPPING\n",
      "✓ 2023_9_16_T191118_1: MYD021KM data already exists (1 files) - SKIPPING\n",
      "✓ 2023_9_16_T191118_2: MYD021KM data already exists (3 files) - SKIPPING\n",
      "✗ 2023_9_16_T191130_1: Missing MYD021KM data\n",
      "    Time: 2023-09-16 19:06 UTC (from AIRS granule 191)\n",
      "    Files: 1 AIRS, 3 EMIT, MYD03: Yes\n",
      "✓ 2023_9_16_T191130_2: MYD021KM data already exists (2 files) - SKIPPING\n",
      "✓ 2023_9_16_T191142: MYD021KM data already exists (3 files) - SKIPPING\n",
      "✓ 2024-09-12-T1955: MYD021KM data already exists (3 files) - SKIPPING\n",
      "✓ 2024-09-12-T2000: MYD021KM data already exists (3 files) - SKIPPING\n",
      "✓ 2024_05_17-T1835: MYD021KM data already exists (1 files) - SKIPPING\n",
      "✓ 2024_11_14_T193337: MYD021KM data already exists (3 files) - SKIPPING\n",
      "✓ 2024_1_12_T185446: MYD021KM data already exists (3 files) - SKIPPING\n",
      "✓ 2024_1_12_T185458: MYD021KM data already exists (3 files) - SKIPPING\n",
      "✓ 2024_1_13_T194646: MYD021KM data already exists (3 files) - SKIPPING\n",
      "✓ 2024_1_13_T194658: MYD021KM data already exists (3 files) - SKIPPING\n",
      "✓ 2024_1_13_T194710: MYD021KM data already exists (3 files) - SKIPPING\n",
      "✓ 2024_2_26_T185437: MYD021KM data already exists (3 files) - SKIPPING\n",
      "✓ 2024_5_16_T193023: MYD021KM data already exists (4 files) - SKIPPING\n",
      "✓ 2024_5_17_T183906: MYD021KM data already exists (3 files) - SKIPPING\n",
      "✓ 2024_5_17_T183918: MYD021KM data already exists (3 files) - SKIPPING\n",
      "✓ 2024_5_17_T183930: MYD021KM data already exists (3 files) - SKIPPING\n",
      "✓ 2024_9_12_T195030: MYD021KM data already exists (3 files) - SKIPPING\n",
      "✓ 2025_11_7_T195446: MYD021KM data already exists (3 files) - SKIPPING\n",
      "✓ 2025_1_13_T195116: MYD021KM data already exists (3 files) - SKIPPING\n",
      "✓ 2025_1_13_T195128: MYD021KM data already exists (3 files) - SKIPPING\n",
      "✓ 2025_3_10_T202437: MYD021KM data already exists (3 files) - SKIPPING\n",
      "======================================================================\n",
      "\n",
      "Found 1 pair(s) missing MYD021KM data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def parse_airs_filename(filename):\n",
    "    \"\"\"\n",
    "    Extract date and time from AIRS filenames.\n",
    "    \n",
    "    Example: AIRS.2024.05.16.193.L2.RetStd.v7.0.7.0.G24137155634.hdf\n",
    "    Format: AIRS.YYYY.MM.DD.HHH where HHH is granule number (0-239)\n",
    "    \n",
    "    Returns:\n",
    "        dict: {'year': int, 'month': int, 'day': int, 'hour': int, 'minute': int}\n",
    "              or None if parsing fails\n",
    "    \"\"\"\n",
    "    # AIRS format: AIRS.YYYY.MM.DD.HHH (where HHH is granule number, ~6 min each)\n",
    "    airs_match = re.search(r'AIRS\\.(\\d{4})\\.(\\d{2})\\.(\\d{2})\\.(\\d{3})', filename)\n",
    "    if airs_match:\n",
    "        year = int(airs_match.group(1))\n",
    "        month = int(airs_match.group(2))\n",
    "        day = int(airs_match.group(3))\n",
    "        granule = int(airs_match.group(4))\n",
    "        \n",
    "        # Convert granule number to approximate UTC time\n",
    "        # AIRS has 240 granules per day (6 minute granules)\n",
    "        minutes_since_midnight = granule * 6\n",
    "        hour = minutes_since_midnight // 60\n",
    "        minute = minutes_since_midnight % 60\n",
    "        \n",
    "        return {\n",
    "            'year': year,\n",
    "            'month': month,\n",
    "            'day': day,\n",
    "            'hour': hour,\n",
    "            'minute': minute,\n",
    "            'granule': granule\n",
    "        }\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def create_temporal_search_string(time_info, window_minutes=5):\n",
    "    \"\"\"\n",
    "    Create CMR temporal search string with a time window around the observation.\n",
    "    \n",
    "    Since AIRS and MODIS are both on Aqua, they observe nearly simultaneously.\n",
    "    We use a smaller window (±10 minutes) compared to AMSR-E.\n",
    "    \n",
    "    Args:\n",
    "        time_info: dict with year, month, day, hour, minute\n",
    "        window_minutes: search window in minutes (default ±10 minutes)\n",
    "    \n",
    "    Returns:\n",
    "        str: CMR temporal search string\n",
    "    \"\"\"\n",
    "    obs_time = dt.datetime(\n",
    "        time_info['year'],\n",
    "        time_info['month'],\n",
    "        time_info['day'],\n",
    "        time_info['hour'],\n",
    "        time_info['minute']\n",
    "    )\n",
    "    \n",
    "    start_time = obs_time - dt.timedelta(minutes=window_minutes)\n",
    "    end_time = obs_time + dt.timedelta(minutes=window_minutes)\n",
    "    \n",
    "    dt_format = '%Y-%m-%dT%H:%M:%SZ'\n",
    "    return start_time.strftime(dt_format) + ',' + end_time.strftime(dt_format)\n",
    "\n",
    "\n",
    "def create_spatial_search_bbox(emit_files):\n",
    "    \"\"\"\n",
    "    Extract bounding box from EMIT filenames if possible.\n",
    "    This can help narrow down MODIS granule searches.\n",
    "    \n",
    "    For now, returns None - spatial filtering can be added if needed.\n",
    "    \"\"\"\n",
    "    # TODO: Could parse EMIT metadata or filenames for spatial bounds\n",
    "    return None\n",
    "\n",
    "\n",
    "# Scan directories for existing data\n",
    "print(\"Scanning data directories...\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "pair_info = {}  # Dictionary to store info for each pair\n",
    "\n",
    "# Get all subdirectories in base_data_dir\n",
    "subdirs = [d for d in Path(base_data_dir).iterdir() if d.is_dir()]\n",
    "\n",
    "for subdir in sorted(subdirs):\n",
    "    pair_name = subdir.name\n",
    "    \n",
    "    # Check if directory has AIRS files but no MYD021KM files\n",
    "    files = list(subdir.glob('*'))\n",
    "    airs_files = [f for f in files if 'AIRS' in f.name and f.suffix in ['.hdf', '.nc']]\n",
    "    myd021km_files = [f for f in files if 'MYD021KM' in f.name and f.suffix in ['.hdf', '.nc']]\n",
    "    myd03_files = [f for f in files if 'MYD03' in f.name and f.suffix in ['.hdf', '.nc']]\n",
    "    emit_files = [f for f in files if 'EMIT' in f.name]\n",
    "    \n",
    "    # Skip if no AIRS files\n",
    "    if not airs_files:\n",
    "        continue\n",
    "    \n",
    "    # Check if MYD021KM already exists\n",
    "    if myd021km_files:\n",
    "        print(f\"✓ {pair_name}: MYD021KM data already exists ({len(myd021km_files)} files) - SKIPPING\")\n",
    "        continue\n",
    "    \n",
    "    # Parse timing from AIRS file\n",
    "    time_info = None\n",
    "    source_file = None\n",
    "    \n",
    "    for airs_file in airs_files:\n",
    "        time_info = parse_airs_filename(airs_file.name)\n",
    "        if time_info:\n",
    "            source_file = airs_file.name\n",
    "            break\n",
    "    \n",
    "    if time_info:\n",
    "        pair_info[pair_name] = {\n",
    "            'directory': subdir,\n",
    "            'time_info': time_info,\n",
    "            'source_file': source_file,\n",
    "            'temporal_str': create_temporal_search_string(time_info),\n",
    "            'airs_count': len(airs_files),\n",
    "            'emit_count': len(emit_files),\n",
    "            'has_myd03': len(myd03_files) > 0\n",
    "        }\n",
    "        \n",
    "        print(f\"✗ {pair_name}: Missing MYD021KM data\")\n",
    "        print(f\"    Time: {time_info['year']:04d}-{time_info['month']:02d}-{time_info['day']:02d} \"\n",
    "              f\"{time_info['hour']:02d}:{time_info['minute']:02d} UTC (from AIRS granule {time_info['granule']})\")\n",
    "        print(f\"    Files: {len(airs_files)} AIRS, {len(emit_files)} EMIT, \"\n",
    "              f\"MYD03: {'Yes' if myd03_files else 'No'}\")\n",
    "    else:\n",
    "        print(f\"⚠ {pair_name}: Could not parse AIRS timing - SKIPPING\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nFound {len(pair_info)} pair(s) missing MYD021KM data\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search and Download MYD021KM Data\n",
    "\n",
    "For each pair missing MYD021KM data, search CMR for coincident MODIS granules and download them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SEARCHING FOR MYD021KM DATA\n",
      "======================================================================\n",
      "\n",
      "Pair: 2023_9_16_T191130_1\n",
      "  Time: 2023-09-16T19:01:00Z,2023-09-16T19:11:00Z\n",
      "  - MYD021KM: Found 3 file(s)\n",
      "  - MYD03: Already present - SKIPPING\n",
      "  Total files to download: 3\n",
      "\n",
      "======================================================================\n",
      "Total pairs with MODIS data found: 1\n",
      "Total files to download: 3\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def search_cmr_modis(concept_id, temporal_str, page_size=2000):\n",
    "    \"\"\"\n",
    "    Search CMR for MODIS granules matching temporal criteria.\n",
    "    \n",
    "    Returns:\n",
    "        list: URLs of matching granules\n",
    "    \"\"\"\n",
    "    granule_search_url = cmrurl + 'granules'\n",
    "    \n",
    "    search_params = {\n",
    "        'concept_id': concept_id,\n",
    "        'temporal': temporal_str,\n",
    "        'page_size': page_size,\n",
    "    }\n",
    "    \n",
    "    headers = {'Accept': 'application/json'}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(granule_search_url, params=search_params, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        granules = response.json()['feed']['entry']\n",
    "        \n",
    "        # Extract data file URLs (exclude metadata and auxiliary files)\n",
    "        urls = []\n",
    "        for g in granules:\n",
    "            file_urls = [\n",
    "                x['href'] for x in g.get('links', [])\n",
    "                if 'https' in x['href']\n",
    "                and any(ext in x['href'] for ext in ['.hdf', '.nc', '.h5', '.he5'])\n",
    "                and '.dmrpp' not in x['href']\n",
    "                and not any(x['href'].endswith(f'.{digit}') for digit in '0123456789')\n",
    "                and not x['href'].endswith(('.xml', '.qa', '.ph', '.html'))\n",
    "            ]\n",
    "            urls.extend(file_urls)\n",
    "        \n",
    "        return urls\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"    Error searching CMR: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# Search for and download MODIS data for each pair\n",
    "print(\"=\" * 70)\n",
    "print(\"SEARCHING FOR MYD021KM DATA\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "download_summary = defaultdict(list)\n",
    "\n",
    "for pair_name, info in pair_info.items():\n",
    "    print(f\"Pair: {pair_name}\")\n",
    "    print(f\"  Time: {info['temporal_str']}\")\n",
    "    \n",
    "    pair_urls = []\n",
    "    \n",
    "    # Search MYD021KM (always needed)\n",
    "    product_key = 'MYD021KM'\n",
    "    product_data = all_products[product_key]\n",
    "    \n",
    "    if not product_data['concept_id']:\n",
    "        print(f\"  - {product_key}: No concept ID available - SKIPPING\")\n",
    "    else:\n",
    "        urls = search_cmr_modis(product_data['concept_id'], info['temporal_str'])\n",
    "        \n",
    "        if urls:\n",
    "            print(f\"  - {product_key}: Found {len(urls)} file(s)\")\n",
    "            pair_urls.extend(urls)\n",
    "        else:\n",
    "            print(f\"  - {product_key}: No files found\")\n",
    "    \n",
    "    # Search MYD03 (geolocation) only if it's not already present\n",
    "    if not info['has_myd03']:\n",
    "        product_key = 'MYD03'\n",
    "        product_data = all_products[product_key]\n",
    "        \n",
    "        if not product_data['concept_id']:\n",
    "            print(f\"  - {product_key}: No concept ID available - SKIPPING\")\n",
    "        else:\n",
    "            urls = search_cmr_modis(product_data['concept_id'], info['temporal_str'])\n",
    "            \n",
    "            if urls:\n",
    "                print(f\"  - {product_key}: Found {len(urls)} file(s)\")\n",
    "                pair_urls.extend(urls)\n",
    "            else:\n",
    "                print(f\"  - {product_key}: No files found\")\n",
    "    else:\n",
    "        print(f\"  - MYD03: Already present - SKIPPING\")\n",
    "    \n",
    "    if pair_urls:\n",
    "        download_summary[pair_name] = {\n",
    "            'urls': pair_urls,\n",
    "            'directory': info['directory'],\n",
    "            'count': len(pair_urls)\n",
    "        }\n",
    "        print(f\"  Total files to download: {len(pair_urls)}\\n\")\n",
    "    else:\n",
    "        print(f\"  ⚠ No MODIS data found for this time period\\n\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total pairs with MODIS data found: {len(download_summary)}\")\n",
    "print(f\"Total files to download: {sum(v['count'] for v in download_summary.values())}\")\n",
    "print(\"=\" * 70)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download MODIS Files\n",
    "\n",
    "Download the identified MYD021KM (and MYD03 if needed) files to their respective pair directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DOWNLOADING MODIS FILES\n",
      "======================================================================\n",
      "\n",
      "Downloading to: 2023_9_16_T191130_1/\n",
      "  Files: 3\n",
      "  ✓ Downloaded 3 file(s)\n",
      "\n",
      "======================================================================\n",
      "DOWNLOAD COMPLETE\n",
      "======================================================================\n",
      "Successfully downloaded: 3 files\n",
      "Data location: /Users/andrewbuggee/Documents/MATLAB/Matlab-Research/Hyperspectral_Cloud_Retrievals/Batch_Scripts/Paper-2/coincident_EMIT_Aqua_data/\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"DOWNLOADING MODIS FILES\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "total_downloaded = 0\n",
    "total_failed = 0\n",
    "\n",
    "for pair_name, download_info in download_summary.items():\n",
    "    pair_dir = download_info['directory']\n",
    "    urls = download_info['urls']\n",
    "    \n",
    "    print(f\"Downloading to: {pair_name}/\")\n",
    "    print(f\"  Files: {len(urls)}\")\n",
    "    \n",
    "    # Create URL file for wget\n",
    "    url_file = pair_dir / 'modis_urls_to_download.txt'\n",
    "    with open(url_file, 'w') as f:\n",
    "        for url in urls:\n",
    "            f.write(url + '\\n')\n",
    "    \n",
    "    # Download using wget\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['wget', '-P', str(pair_dir), '-i', str(url_file)],\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        \n",
    "        # Count successful downloads\n",
    "        if result.stderr:\n",
    "            saved_count = result.stderr.count('saved')\n",
    "            total_downloaded += saved_count\n",
    "            print(f\"  ✓ Downloaded {saved_count} file(s)\")\n",
    "        \n",
    "        # Clean up URL file if successful\n",
    "        if result.returncode == 0:\n",
    "            url_file.unlink()\n",
    "        else:\n",
    "            total_failed += len(urls)\n",
    "            print(f\"  ⚠ Download completed with warnings (return code: {result.returncode})\")\n",
    "            print(f\"    URL file saved: {url_file.name}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        total_failed += len(urls)\n",
    "        print(f\"  ✗ Error downloading: {e}\")\n",
    "        print(f\"    URLs saved to: {url_file.name}\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DOWNLOAD COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Successfully downloaded: {total_downloaded} files\")\n",
    "if total_failed > 0:\n",
    "    print(f\"Failed/warnings: {total_failed} files\")\n",
    "print(f\"Data location: {base_data_dir}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The notebook has:\n",
    "1. Scanned your existing coincident data directories\n",
    "2. Identified pairs missing MYD021KM (MODIS calibrated radiance) data\n",
    "3. Extracted timing information from AIRS filenames\n",
    "4. Searched NASA CMR for matching MODIS granules (±5 minute window)\n",
    "5. Downloaded MYD021KM files (and MYD03 geolocation if needed) to the appropriate directories\n",
    "\n",
    "**About MYD021KM:**\n",
    "- MODIS Level-1B calibrated radiances at 1km resolution\n",
    "- Contains reflectance and emissive bands for atmospheric/surface studies\n",
    "- Often used with MYD03 (geolocation) for precise georeferencing\n",
    "\n",
    "**Note:** If you encounter download issues, check:\n",
    "- Your `.netrc` file has correct NASA Earthdata credentials\n",
    "- File permissions: `chmod 600 ~/.netrc`\n",
    "- Any `*_urls_to_download.txt` files left in directories indicate partial downloads that can be retried manually"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
